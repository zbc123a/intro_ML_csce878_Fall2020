{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('mywork': conda)",
   "display_name": "Python 3.7.6 64-bit ('mywork': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5948cee07a8bd862e872a8ad68bfefad01def149c91ad75defc90aea373ecbbd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Naive Bayes Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multinomial Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multinomial_NB:\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        '''Initialization function to instantiate the class.'''\n",
    "        self.alpha = alpha\n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        Input\n",
    "        X: ndarray, A numpy array with rows representing data samples and columns\n",
    "        representing numerical features.\n",
    "        Y: ndarray, A 1D numpy array with labels corresponding to each row of the feature\n",
    "        matrix X.\n",
    "        '''\n",
    "        # define basic parameters\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        self.classes = np.unique(Y,return_counts = True)\n",
    "        self.classes_num = len(self.classes[0]) # the number of classes\n",
    "        N, J = X.shape # J - feature, the number of words, N - the number of sample size\n",
    "        #alpha: parameteres of the distirbution\n",
    "        sum_alpha_prior = self.classes_num*alpha\n",
    "        # calculate the prior probabilty\n",
    "        for i in range(self.classes_num):\n",
    "            counts_classes = self.classes[1][i]\n",
    "            self.pi[i] = (self.alpha+counts_classes)/(N+sum_alpha_prior)\n",
    "        # calculate the conditional probabilty\n",
    "        sum_alpha_likelihood = J*alpha\n",
    "        for i in range(self.classes_num):\n",
    "            X_i = X[np.nonzeros(Y==self.classes[0][i])[0],:]\n",
    "            M_J_i = X_i.sum(axis = 0)\n",
    "            M_i = M_J_i.sum()\n",
    "            for j in range(J):\n",
    "                self.theta[i][j] = (self.alpha+M_J_i)/(sum_alpha_likelihood+M_i)\n",
    "    def predit(self,X):\n",
    "        '''\n",
    "        This method performs classification on an array of test vectors X. \n",
    "        Returns:\n",
    "        1D array (column vector) of predictions for each row in X.\n",
    "        '''\n",
    "        pred = self.predict_log_proba(X)\n",
    "        y_pred = np.zeros(X.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            # prediction by row (sample)\n",
    "            y_pred[i] = self.classes[0][np.argmax(pred[i])]\n",
    "        return y_pred\n",
    "    def predict_log_proba(self,X):\n",
    "        '''\n",
    "        This method returns log-probability estimates for the test matrix X.\n",
    "        Arguments:\n",
    "        X : ndarray\n",
    "        A numpy array containing samples to be used for prediction. Its rows\n",
    "        represent data samples and columns represent numerical features.\n",
    "        Returns:\n",
    "        A numpy array that contains log-probability of the samples (unnormalized\n",
    "        log posteriors) for each class in the model. The number rows are equal to\n",
    "        the rows in X and number of columns are equal to the number of classes.\n",
    "        '''\n",
    "        N, J = X.shape # J - feature, the number of words, N - the number of sample size\n",
    "        classes_num = len(self.classes[0]) #the number of classess\n",
    "        prob = np.zeros(shape=(N, classes_num)) #probabilty matrix\n",
    "        for k in range(classes_num): #in this case, its 2\n",
    "            for i in range(N):\n",
    "                summation = 0\n",
    "                for j in range(J):\n",
    "                    summation += np.log(self.theta[k,j]**X[i,j])\n",
    "                prob[i,k] = np.log(self.pi[k])+summation\n",
    "        return prob\n",
    "    def predict_proba(self,X):\n",
    "        N, J = X.shape # J - feature, the number of words, N - the number of sample size\n",
    "        classes_num = len(self.classes[0]) #the number of classess\n",
    "        prob = np.zeros(shape=(N, classes_num)) #probabilty matrix\n",
    "        \n",
    "        for k in range(classes_num):\n",
    "            for i in range(N):\n",
    "                p = 1\n",
    "                for j in range(J):\n",
    "                    p *= self.theta[k,j]**X[i,j]\n",
    "                prob[i,k] = self.pi[k]*p\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}